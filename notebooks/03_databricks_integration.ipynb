{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Integration with Pydantic\n",
    "\n",
    "This notebook demonstrates how to integrate Pydantic models with Databricks workflows for robust data processing, validation, and machine learning pipelines.\n",
    "\n",
    "## Learning Objectives\n",
    "- Use Pydantic models in Databricks environments\n",
    "- Implement data validation in ETL pipelines\n",
    "- Create type-safe data transformations\n",
    "- Build ML feature stores with validated schemas\n",
    "- Handle data quality and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Databricks environment\n",
    "# %pip install pydantic pandas numpy scikit-learn\n",
    "# Note: In actual Databricks, use %pip install or dbutils.library.installPyPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Pydantic Models for Data Pipeline\n",
    "\n",
    "Define data schemas that ensure consistency across your data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "from datetime import datetime, date\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "\n",
    "# Base model for audit tracking\n",
    "class DataPipelineModel(BaseModel):\n",
    "    \"\"\"Base model for all data pipeline entities\"\"\"\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    source_system: str = Field(default=\"databricks\")\n",
    "    data_quality_score: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "    \n",
    "    class Config:\n",
    "        json_encoders = {\n",
    "            datetime: lambda dt: dt.isoformat(),\n",
    "            date: lambda d: d.isoformat()\n",
    "        }\n",
    "\n",
    "# Raw data models\n",
    "class CustomerRaw(DataPipelineModel):\n",
    "    \"\"\"Raw customer data from source systems\"\"\"\n",
    "    customer_id: str\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    email: str\n",
    "    phone: Optional[str] = None\n",
    "    birth_date: Optional[date] = None\n",
    "    registration_date: date\n",
    "    country: str\n",
    "    \n",
    "    @validator('email')\n",
    "    def validate_email(cls, v):\n",
    "        if '@' not in v or '.' not in v.split('@')[1]:\n",
    "            raise ValueError('Invalid email format')\n",
    "        return v.lower().strip()\n",
    "    \n",
    "    @validator('customer_id')\n",
    "    def validate_customer_id(cls, v):\n",
    "        if not v or len(v) < 3:\n",
    "            raise ValueError('Customer ID must be at least 3 characters')\n",
    "        return v.strip().upper()\n",
    "\n",
    "class TransactionRaw(DataPipelineModel):\n",
    "    \"\"\"Raw transaction data\"\"\"\n",
    "    transaction_id: str\n",
    "    customer_id: str\n",
    "    product_id: str\n",
    "    quantity: int = Field(gt=0)\n",
    "    unit_price: float = Field(gt=0)\n",
    "    transaction_date: datetime\n",
    "    currency: str = Field(default=\"USD\")\n",
    "    payment_method: str\n",
    "    discount_amount: float = Field(default=0.0, ge=0)\n",
    "    \n",
    "    @property\n",
    "    def total_amount(self) -> float:\n",
    "        return (self.quantity * self.unit_price) - self.discount_amount\n",
    "    \n",
    "    @validator('currency')\n",
    "    def validate_currency(cls, v):\n",
    "        valid_currencies = ['USD', 'EUR', 'GBP', 'JPY', 'CAD']\n",
    "        if v.upper() not in valid_currencies:\n",
    "            raise ValueError(f'Currency must be one of {valid_currencies}')\n",
    "        return v.upper()\n",
    "\n",
    "print(\"Data models defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion and Validation\n",
    "\n",
    "Implement robust data ingestion with validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "from typing import Tuple, List\n",
    "import json\n",
    "\n",
    "class DataIngestionPipeline:\n",
    "    \"\"\"Data ingestion pipeline with Pydantic validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_errors = []\n",
    "        self.processed_records = 0\n",
    "        self.failed_records = 0\n",
    "    \n",
    "    def validate_and_transform_customers(self, raw_data: List[Dict]) -> Tuple[List[CustomerRaw], List[Dict]]:\n",
    "        \"\"\"Validate customer data and return valid/invalid records\"\"\"\n",
    "        valid_customers = []\n",
    "        invalid_records = []\n",
    "        \n",
    "        for i, record in enumerate(raw_data):\n",
    "            try:\n",
    "                # Validate and create customer instance\n",
    "                customer = CustomerRaw(**record)\n",
    "                \n",
    "                # Additional business logic validation\n",
    "                if customer.birth_date and customer.birth_date > date.today():\n",
    "                    raise ValueError(\"Birth date cannot be in the future\")\n",
    "                \n",
    "                # Calculate data quality score\n",
    "                quality_score = self._calculate_customer_quality_score(customer)\n",
    "                customer.data_quality_score = quality_score\n",
    "                \n",
    "                valid_customers.append(customer)\n",
    "                self.processed_records += 1\n",
    "                \n",
    "            except ValidationError as e:\n",
    "                error_info = {\n",
    "                    'record_index': i,\n",
    "                    'raw_data': record,\n",
    "                    'errors': [f\"{error['loc'][0]}: {error['msg']}\" for error in e.errors()],\n",
    "                    'error_type': 'validation'\n",
    "                }\n",
    "                invalid_records.append(error_info)\n",
    "                self.failed_records += 1\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    'record_index': i,\n",
    "                    'raw_data': record,\n",
    "                    'errors': [str(e)],\n",
    "                    'error_type': 'business_logic'\n",
    "                }\n",
    "                invalid_records.append(error_info)\n",
    "                self.failed_records += 1\n",
    "        \n",
    "        return valid_customers, invalid_records\n",
    "    \n",
    "    def _calculate_customer_quality_score(self, customer: CustomerRaw) -> float:\n",
    "        \"\"\"Calculate data quality score for customer record\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Email validation (0.3)\n",
    "        if customer.email and '@' in customer.email:\n",
    "            score += 0.3\n",
    "        \n",
    "        # Phone number (0.2)\n",
    "        if customer.phone and len(customer.phone.strip()) >= 10:\n",
    "            score += 0.2\n",
    "        \n",
    "        # Birth date (0.2)\n",
    "        if customer.birth_date:\n",
    "            score += 0.2\n",
    "        \n",
    "        # Name completeness (0.3)\n",
    "        if customer.first_name and customer.last_name:\n",
    "            if len(customer.first_name.strip()) > 1 and len(customer.last_name.strip()) > 1:\n",
    "                score += 0.3\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def get_pipeline_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get pipeline processing statistics\"\"\"\n",
    "        total = self.processed_records + self.failed_records\n",
    "        success_rate = (self.processed_records / total) * 100 if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_records': total,\n",
    "            'successful': self.processed_records,\n",
    "            'failed': self.failed_records,\n",
    "            'success_rate': round(success_rate, 2)\n",
    "        }\n",
    "\n",
    "# Sample raw customer data (simulating data from various sources)\n",
    "raw_customer_data = [\n",
    "    {\n",
    "        \"customer_id\": \"CUST001\",\n",
    "        \"first_name\": \"Alice\",\n",
    "        \"last_name\": \"Johnson\",\n",
    "        \"email\": \"alice.johnson@email.com\",\n",
    "        \"phone\": \"+1-555-123-4567\",\n",
    "        \"birth_date\": \"1985-03-15\",\n",
    "        \"registration_date\": \"2023-01-10\",\n",
    "        \"country\": \"USA\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"CUST002\",\n",
    "        \"first_name\": \"Bob\",\n",
    "        \"last_name\": \"Smith\",\n",
    "        \"email\": \"bob@email.com\",\n",
    "        \"registration_date\": \"2023-02-01\",\n",
    "        \"country\": \"Canada\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"\",  # Invalid - empty ID\n",
    "        \"first_name\": \"Charlie\",\n",
    "        \"last_name\": \"Brown\",\n",
    "        \"email\": \"invalid-email\",  # Invalid email\n",
    "        \"registration_date\": \"2023-03-01\",\n",
    "        \"country\": \"UK\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"CUST004\",\n",
    "        \"first_name\": \"Diana\",\n",
    "        \"last_name\": \"Wilson\",\n",
    "        \"email\": \"diana@email.com\",\n",
    "        \"phone\": \"555-9876\",\n",
    "        \"birth_date\": \"1990-07-22\",\n",
    "        \"registration_date\": \"2023-04-15\",\n",
    "        \"country\": \"Australia\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process the data\n",
    "pipeline = DataIngestionPipeline()\n",
    "valid_customers, invalid_customers = pipeline.validate_and_transform_customers(raw_customer_data)\n",
    "\n",
    "print(f\"Data Ingestion Results:\")\n",
    "print(f\"Valid customers: {len(valid_customers)}\")\n",
    "print(f\"Invalid records: {len(invalid_customers)}\")\n",
    "print(f\"Pipeline stats: {pipeline.get_pipeline_stats()}\")\n",
    "\n",
    "print(\"\\nValid Customers:\")\n",
    "for customer in valid_customers:\n",
    "    print(f\"  - {customer.customer_id}: {customer.first_name} {customer.last_name} (Quality: {customer.data_quality_score:.1f})\")\n",
    "\n",
    "print(\"\\nInvalid Records:\")\n",
    "for invalid in invalid_customers:\n",
    "    print(f\"  - Record {invalid['record_index']}: {'; '.join(invalid['errors'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering with Pydantic\n",
    "\n",
    "Create feature engineering pipelines with validated schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "class CustomerFeatures(BaseModel):\n",
    "    \"\"\"Engineered features for customer analytics\"\"\"\n",
    "    customer_id: str\n",
    "    \n",
    "    # Demographic features\n",
    "    age: Optional[int] = Field(None, ge=0, le=150)\n",
    "    age_group: Optional[str] = None\n",
    "    country: str\n",
    "    \n",
    "    # Engagement features\n",
    "    days_since_registration: int = Field(ge=0)\n",
    "    registration_month: int = Field(ge=1, le=12)\n",
    "    registration_year: int = Field(ge=1900, le=2030)\n",
    "    \n",
    "    # Data quality features\n",
    "    has_phone: bool\n",
    "    has_birth_date: bool\n",
    "    data_completeness_score: float = Field(ge=0.0, le=1.0)\n",
    "    \n",
    "    # Computed fields\n",
    "    is_new_customer: bool = False  # < 30 days since registration\n",
    "    customer_tier: str = \"bronze\"  # bronze, silver, gold based on completeness\n",
    "    \n",
    "    @validator('age_group', pre=False, always=True)\n",
    "    def set_age_group(cls, v, values):\n",
    "        age = values.get('age')\n",
    "        if age is None:\n",
    "            return \"unknown\"\n",
    "        elif age < 25:\n",
    "            return \"young\"\n",
    "        elif age < 45:\n",
    "            return \"middle\"\n",
    "        elif age < 65:\n",
    "            return \"mature\"\n",
    "        else:\n",
    "            return \"senior\"\n",
    "    \n",
    "    @validator('customer_tier', pre=False, always=True)\n",
    "    def set_customer_tier(cls, v, values):\n",
    "        score = values.get('data_completeness_score', 0)\n",
    "        if score >= 0.8:\n",
    "            return \"gold\"\n",
    "        elif score >= 0.6:\n",
    "            return \"silver\"\n",
    "        else:\n",
    "            return \"bronze\"\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Feature engineering pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_date: date = None):\n",
    "        self.reference_date = reference_date or date.today()\n",
    "    \n",
    "    def engineer_customer_features(self, customers: List[CustomerRaw]) -> List[CustomerFeatures]:\n",
    "        \"\"\"Generate features from customer data\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for customer in customers:\n",
    "            # Calculate age\n",
    "            age = None\n",
    "            if customer.birth_date:\n",
    "                age = self._calculate_age(customer.birth_date)\n",
    "            \n",
    "            # Calculate days since registration\n",
    "            days_since_reg = (self.reference_date - customer.registration_date).days\n",
    "            \n",
    "            # Create features\n",
    "            feature_record = CustomerFeatures(\n",
    "                customer_id=customer.customer_id,\n",
    "                age=age,\n",
    "                country=customer.country,\n",
    "                days_since_registration=days_since_reg,\n",
    "                registration_month=customer.registration_date.month,\n",
    "                registration_year=customer.registration_date.year,\n",
    "                has_phone=bool(customer.phone),\n",
    "                has_birth_date=bool(customer.birth_date),\n",
    "                data_completeness_score=customer.data_quality_score or 0.0,\n",
    "                is_new_customer=days_since_reg <= 30\n",
    "            )\n",
    "            \n",
    "            features.append(feature_record)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_age(self, birth_date: date) -> int:\n",
    "        \"\"\"Calculate age from birth date\"\"\"\n",
    "        today = self.reference_date\n",
    "        age = today.year - birth_date.year\n",
    "        \n",
    "        # Adjust if birthday hasn't occurred this year\n",
    "        if today.month < birth_date.month or (today.month == birth_date.month and today.day < birth_date.day):\n",
    "            age -= 1\n",
    "            \n",
    "        return max(0, age)\n",
    "\n",
    "# Generate features\n",
    "feature_engineer = FeatureEngineer()\n",
    "customer_features = feature_engineer.engineer_customer_features(valid_customers)\n",
    "\n",
    "print(\"Customer Features Generated:\")\n",
    "print(f\"Total features: {len(customer_features)}\")\n",
    "\n",
    "for features in customer_features:\n",
    "    print(f\"\\n{features.customer_id}:\")\n",
    "    print(f\"  Age: {features.age} ({features.age_group})\")\n",
    "    print(f\"  Days since registration: {features.days_since_registration}\")\n",
    "    print(f\"  Data completeness: {features.data_completeness_score:.1f}\")\n",
    "    print(f\"  Customer tier: {features.customer_tier}\")\n",
    "    print(f\"  New customer: {features.is_new_customer}\")\n",
    "\n",
    "# Convert to DataFrame for further processing\n",
    "features_df_data = [feature.dict() for feature in customer_features]\n",
    "print(f\"\\nFeatures ready for DataFrame: {len(features_df_data)} records\")\n",
    "print(f\"Feature columns: {list(features_df_data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Monitoring\n",
    "\n",
    "Implement data quality monitoring with Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Counter\n",
    "\n",
    "class DataQualityMetrics(BaseModel):\n",
    "    \"\"\"Data quality metrics for monitoring\"\"\"\n",
    "    dataset_name: str\n",
    "    total_records: int = Field(ge=0)\n",
    "    valid_records: int = Field(ge=0)\n",
    "    invalid_records: int = Field(ge=0)\n",
    "    \n",
    "    # Quality scores\n",
    "    average_quality_score: float = Field(ge=0.0, le=1.0)\n",
    "    completeness_rate: float = Field(ge=0.0, le=1.0)\n",
    "    validity_rate: float = Field(ge=0.0, le=1.0)\n",
    "    \n",
    "    # Field-specific metrics\n",
    "    field_completeness: Dict[str, float] = {}\n",
    "    field_validity: Dict[str, float] = {}\n",
    "    \n",
    "    # Error analysis\n",
    "    common_errors: List[Dict[str, Any]] = []\n",
    "    error_categories: Dict[str, int] = {}\n",
    "    \n",
    "    # Timestamp\n",
    "    measured_at: datetime = Field(default_factory=datetime.now)\n",
    "    \n",
    "    @validator('validity_rate', pre=False, always=True)\n",
    "    def calculate_validity_rate(cls, v, values):\n",
    "        total = values.get('total_records', 0)\n",
    "        valid = values.get('valid_records', 0)\n",
    "        return (valid / total) if total > 0 else 0.0\n",
    "    \n",
    "    class Config:\n",
    "        json_encoders = {\n",
    "            datetime: lambda dt: dt.isoformat()\n",
    "        }\n",
    "\n",
    "class DataQualityMonitor:\n",
    "    \"\"\"Monitor data quality across pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.quality_history = []\n",
    "    \n",
    "    def analyze_customer_quality(self, \n",
    "                                valid_customers: List[CustomerRaw], \n",
    "                                invalid_records: List[Dict]) -> DataQualityMetrics:\n",
    "        \"\"\"Analyze data quality for customer dataset\"\"\"\n",
    "        \n",
    "        total_records = len(valid_customers) + len(invalid_records)\n",
    "        valid_count = len(valid_customers)\n",
    "        invalid_count = len(invalid_records)\n",
    "        \n",
    "        # Calculate average quality score\n",
    "        avg_quality = 0.0\n",
    "        if valid_customers:\n",
    "            total_quality = sum(c.data_quality_score or 0 for c in valid_customers)\n",
    "            avg_quality = total_quality / len(valid_customers)\n",
    "        \n",
    "        # Field completeness analysis\n",
    "        field_completeness = self._analyze_field_completeness(valid_customers)\n",
    "        \n",
    "        # Error analysis\n",
    "        error_analysis = self._analyze_errors(invalid_records)\n",
    "        \n",
    "        # Calculate overall completeness\n",
    "        completeness_rate = sum(field_completeness.values()) / len(field_completeness) if field_completeness else 0.0\n",
    "        \n",
    "        metrics = DataQualityMetrics(\n",
    "            dataset_name=\"customers\",\n",
    "            total_records=total_records,\n",
    "            valid_records=valid_count,\n",
    "            invalid_records=invalid_count,\n",
    "            average_quality_score=avg_quality,\n",
    "            completeness_rate=completeness_rate,\n",
    "            field_completeness=field_completeness,\n",
    "            common_errors=error_analysis['common_errors'],\n",
    "            error_categories=error_analysis['error_categories']\n",
    "        )\n",
    "        \n",
    "        self.quality_history.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def _analyze_field_completeness(self, customers: List[CustomerRaw]) -> Dict[str, float]:\n",
    "        \"\"\"Analyze completeness of each field\"\"\"\n",
    "        if not customers:\n",
    "            return {}\n",
    "        \n",
    "        field_counts = defaultdict(int)\n",
    "        total = len(customers)\n",
    "        \n",
    "        for customer in customers:\n",
    "            # Check each field for completeness\n",
    "            if customer.email:\n",
    "                field_counts['email'] += 1\n",
    "            if customer.phone:\n",
    "                field_counts['phone'] += 1\n",
    "            if customer.birth_date:\n",
    "                field_counts['birth_date'] += 1\n",
    "            if customer.first_name:\n",
    "                field_counts['first_name'] += 1\n",
    "            if customer.last_name:\n",
    "                field_counts['last_name'] += 1\n",
    "            if customer.country:\n",
    "                field_counts['country'] += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        return {field: (count / total) for field, count in field_counts.items()}\n",
    "    \n",
    "    def _analyze_errors(self, invalid_records: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze common errors and patterns\"\"\"\n",
    "        error_counts = defaultdict(int)\n",
    "        error_categories = defaultdict(int)\n",
    "        \n",
    "        for record in invalid_records:\n",
    "            error_type = record.get('error_type', 'unknown')\n",
    "            error_categories[error_type] += 1\n",
    "            \n",
    "            for error in record.get('errors', []):\n",
    "                error_counts[error] += 1\n",
    "        \n",
    "        # Get top 5 most common errors\n",
    "        common_errors = [\n",
    "            {'error': error, 'count': count} \n",
    "            for error, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'common_errors': common_errors,\n",
    "            'error_categories': dict(error_categories)\n",
    "        }\n",
    "    \n",
    "    def generate_quality_report(self, metrics: DataQualityMetrics) -> str:\n",
    "        \"\"\"Generate a human-readable quality report\"\"\"\n",
    "        report = f\"\"\"\n",
    "=== DATA QUALITY REPORT ===\n",
    "Dataset: {metrics.dataset_name}\n",
    "Measured at: {metrics.measured_at.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "OVERALL METRICS:\n",
    "  Total Records: {metrics.total_records:,}\n",
    "  Valid Records: {metrics.valid_records:,} ({metrics.validity_rate:.1%})\n",
    "  Invalid Records: {metrics.invalid_records:,}\n",
    "  Average Quality Score: {metrics.average_quality_score:.2f}\n",
    "  Data Completeness: {metrics.completeness_rate:.1%}\n",
    "\n",
    "FIELD COMPLETENESS:\n",
    "\"\"\"\n",
    "        for field, rate in metrics.field_completeness.items():\n",
    "            report += f\"  {field}: {rate:.1%}\\n\"\n",
    "        \n",
    "        if metrics.common_errors:\n",
    "            report += \"\\nCOMMON ERRORS:\\n\"\n",
    "            for error in metrics.common_errors:\n",
    "                report += f\"  - {error['error']} ({error['count']} times)\\n\"\n",
    "        \n",
    "        if metrics.error_categories:\n",
    "            report += \"\\nERROR CATEGORIES:\\n\"\n",
    "            for category, count in metrics.error_categories.items():\n",
    "                report += f\"  - {category}: {count}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Monitor data quality\n",
    "monitor = DataQualityMonitor()\n",
    "quality_metrics = monitor.analyze_customer_quality(valid_customers, invalid_customers)\n",
    "\n",
    "# Generate and print report\n",
    "quality_report = monitor.generate_quality_report(quality_metrics)\n",
    "print(quality_report)\n",
    "\n",
    "# Quality metrics as JSON (for storing in data lake)\n",
    "print(\"\\nQuality Metrics JSON:\")\n",
    "print(quality_metrics.json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ML Feature Store Integration\n",
    "\n",
    "Create ML-ready feature stores with Pydantic validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "class MLFeatureVector(BaseModel):\n",
    "    \"\"\"ML-ready feature vector\"\"\"\n",
    "    customer_id: str\n",
    "    \n",
    "    # Numerical features\n",
    "    age_normalized: float = Field(ge=0.0, le=1.0)\n",
    "    days_since_registration_log: float\n",
    "    data_completeness_score: float = Field(ge=0.0, le=1.0)\n",
    "    \n",
    "    # Categorical features (encoded)\n",
    "    country_encoded: int = Field(ge=0)\n",
    "    age_group_encoded: int = Field(ge=0)\n",
    "    customer_tier_encoded: int = Field(ge=0)\n",
    "    \n",
    "    # Binary features\n",
    "    has_phone: int = Field(ge=0, le=1)\n",
    "    has_birth_date: int = Field(ge=0, le=1)\n",
    "    is_new_customer: int = Field(ge=0, le=1)\n",
    "    \n",
    "    # Feature metadata\n",
    "    feature_version: str = \"v1.0\"\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    \n",
    "    def to_array(self) -> np.ndarray:\n",
    "        \"\"\"Convert to numpy array for ML models\"\"\"\n",
    "        return np.array([\n",
    "            self.age_normalized,\n",
    "            self.days_since_registration_log,\n",
    "            self.data_completeness_score,\n",
    "            self.country_encoded,\n",
    "            self.age_group_encoded,\n",
    "            self.customer_tier_encoded,\n",
    "            self.has_phone,\n",
    "            self.has_birth_date,\n",
    "            self.is_new_customer\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_names(cls) -> List[str]:\n",
    "        \"\"\"Get feature names for ML models\"\"\"\n",
    "        return [\n",
    "            'age_normalized',\n",
    "            'days_since_registration_log',\n",
    "            'data_completeness_score',\n",
    "            'country_encoded',\n",
    "            'age_group_encoded',\n",
    "            'customer_tier_encoded',\n",
    "            'has_phone',\n",
    "            'has_birth_date',\n",
    "            'is_new_customer'\n",
    "        ]\n",
    "\n",
    "class FeatureStore:\n",
    "    \"\"\"ML Feature Store with validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_stats = {}\n",
    "    \n",
    "    def prepare_ml_features(self, features: List[CustomerFeatures]) -> List[MLFeatureVector]:\n",
    "        \"\"\"Transform customer features to ML-ready features\"\"\"\n",
    "        \n",
    "        # Extract data for preprocessing\n",
    "        ages = [f.age for f in features if f.age is not None]\n",
    "        days_reg = [f.days_since_registration for f in features]\n",
    "        countries = [f.country for f in features]\n",
    "        age_groups = [f.age_group for f in features]\n",
    "        tiers = [f.customer_tier for f in features]\n",
    "        \n",
    "        # Fit encoders and scalers\n",
    "        if 'country' not in self.encoders:\n",
    "            self.encoders['country'] = LabelEncoder()\n",
    "            self.encoders['country'].fit(countries)\n",
    "        \n",
    "        if 'age_group' not in self.encoders:\n",
    "            self.encoders['age_group'] = LabelEncoder()\n",
    "            self.encoders['age_group'].fit(age_groups)\n",
    "        \n",
    "        if 'customer_tier' not in self.encoders:\n",
    "            self.encoders['customer_tier'] = LabelEncoder()\n",
    "            self.encoders['customer_tier'].fit(tiers)\n",
    "        \n",
    "        # Age normalization (0-100 years -> 0-1)\n",
    "        if ages:\n",
    "            self.feature_stats['age_max'] = max(100, max(ages))\n",
    "        else:\n",
    "            self.feature_stats['age_max'] = 100\n",
    "        \n",
    "        # Create ML feature vectors\n",
    "        ml_features = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Normalize age\n",
    "            age_norm = (feature.age / self.feature_stats['age_max']) if feature.age else 0.0\n",
    "            \n",
    "            # Log transform days since registration\n",
    "            days_log = np.log1p(feature.days_since_registration)\n",
    "            \n",
    "            # Encode categorical variables\n",
    "            country_enc = self.encoders['country'].transform([feature.country])[0]\n",
    "            age_group_enc = self.encoders['age_group'].transform([feature.age_group])[0]\n",
    "            tier_enc = self.encoders['customer_tier'].transform([feature.customer_tier])[0]\n",
    "            \n",
    "            # Create ML feature vector\n",
    "            ml_feature = MLFeatureVector(\n",
    "                customer_id=feature.customer_id,\n",
    "                age_normalized=age_norm,\n",
    "                days_since_registration_log=days_log,\n",
    "                data_completeness_score=feature.data_completeness_score,\n",
    "                country_encoded=int(country_enc),\n",
    "                age_group_encoded=int(age_group_enc),\n",
    "                customer_tier_encoded=int(tier_enc),\n",
    "                has_phone=int(feature.has_phone),\n",
    "                has_birth_date=int(feature.has_birth_date),\n",
    "                is_new_customer=int(feature.is_new_customer)\n",
    "            )\n",
    "            \n",
    "            ml_features.append(ml_feature)\n",
    "        \n",
    "        return ml_features\n",
    "    \n",
    "    def create_training_dataset(self, ml_features: List[MLFeatureVector]) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Create training dataset from ML features\"\"\"\n",
    "        feature_matrix = np.array([f.to_array() for f in ml_features])\n",
    "        feature_names = MLFeatureVector.get_feature_names()\n",
    "        customer_ids = [f.customer_id for f in ml_features]\n",
    "        \n",
    "        return feature_matrix, feature_names, customer_ids\n",
    "    \n",
    "    def get_feature_stats(self, ml_features: List[MLFeatureVector]) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate feature statistics\"\"\"\n",
    "        feature_matrix, feature_names, _ = self.create_training_dataset(ml_features)\n",
    "        \n",
    "        stats = {}\n",
    "        for i, name in enumerate(feature_names):\n",
    "            column = feature_matrix[:, i]\n",
    "            stats[name] = {\n",
    "                'mean': float(np.mean(column)),\n",
    "                'std': float(np.std(column)),\n",
    "                'min': float(np.min(column)),\n",
    "                'max': float(np.max(column)),\n",
    "                'null_count': int(np.sum(np.isnan(column)))\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Create feature store and prepare ML features\n",
    "feature_store = FeatureStore()\n",
    "ml_features = feature_store.prepare_ml_features(customer_features)\n",
    "\n",
    "print(f\"ML Features prepared: {len(ml_features)}\")\n",
    "\n",
    "# Create training dataset\n",
    "X, feature_names, customer_ids = feature_store.create_training_dataset(ml_features)\n",
    "print(f\"\\nTraining matrix shape: {X.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample ML Features:\")\n",
    "for i, ml_feat in enumerate(ml_features[:2]):\n",
    "    print(f\"\\nCustomer {ml_feat.customer_id}:\")\n",
    "    array_repr = ml_feat.to_array()\n",
    "    for j, name in enumerate(feature_names):\n",
    "        print(f\"  {name}: {array_repr[j]:.4f}\")\n",
    "\n",
    "# Feature statistics\n",
    "feature_stats = feature_store.get_feature_stats(ml_features)\n",
    "print(\"\\nFeature Statistics:\")\n",
    "for name, stats in feature_stats.items():\n",
    "    print(f\"  {name}: mean={stats['mean']:.3f}, std={stats['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Databricks-Specific Integration Patterns\n",
    "\n",
    "Patterns for using Pydantic in Databricks notebooks and jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks-specific utilities\n",
    "class DatabricksConfig(BaseModel):\n",
    "    \"\"\"Configuration for Databricks jobs\"\"\"\n",
    "    job_name: str\n",
    "    cluster_size: str = \"small\"\n",
    "    timeout_minutes: int = Field(default=60, gt=0)\n",
    "    max_retries: int = Field(default=3, ge=0)\n",
    "    \n",
    "    # Data paths\n",
    "    input_path: str\n",
    "    output_path: str\n",
    "    checkpoint_path: Optional[str] = None\n",
    "    \n",
    "    # Quality thresholds\n",
    "    min_quality_score: float = Field(default=0.7, ge=0.0, le=1.0)\n",
    "    max_error_rate: float = Field(default=0.1, ge=0.0, le=1.0)\n",
    "    \n",
    "    @validator('input_path', 'output_path')\n",
    "    def validate_paths(cls, v):\n",
    "        if not v.startswith(('/dbfs/', 's3://', 'abfss://')):\n",
    "            raise ValueError('Path must be valid Databricks path')\n",
    "        return v\n",
    "\n",
    "class DatabricksJobResult(BaseModel):\n",
    "    \"\"\"Result of Databricks job execution\"\"\"\n",
    "    job_name: str\n",
    "    status: str = Field(regex=r'^(success|failed|running|cancelled)$')\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    \n",
    "    # Processing metrics\n",
    "    records_processed: int = Field(ge=0)\n",
    "    records_failed: int = Field(ge=0)\n",
    "    processing_time_seconds: Optional[float] = None\n",
    "    \n",
    "    # Quality metrics\n",
    "    average_quality_score: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "    error_rate: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "    \n",
    "    # Output information\n",
    "    output_location: Optional[str] = None\n",
    "    output_format: str = \"parquet\"\n",
    "    \n",
    "    # Error information\n",
    "    error_message: Optional[str] = None\n",
    "    stack_trace: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.records_processed + self.records_failed\n",
    "        return (self.records_processed / total) if total > 0 else 0.0\n",
    "    \n",
    "    @validator('end_time')\n",
    "    def validate_end_time(cls, v, values):\n",
    "        start_time = values.get('start_time')\n",
    "        if v and start_time and v < start_time:\n",
    "            raise ValueError('End time cannot be before start time')\n",
    "        return v\n",
    "\n",
    "def simulate_databricks_job(config: DatabricksConfig) -> DatabricksJobResult:\n",
    "    \"\"\"Simulate a Databricks job execution\"\"\"\n",
    "    import time\n",
    "    import random\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Simulate processing\n",
    "    time.sleep(0.1)  # Simulate processing time\n",
    "    \n",
    "    # Simulate results\n",
    "    total_records = random.randint(1000, 10000)\n",
    "    failed_records = int(total_records * random.uniform(0, config.max_error_rate))\n",
    "    processed_records = total_records - failed_records\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    # Determine status based on quality thresholds\n",
    "    error_rate = failed_records / total_records\n",
    "    avg_quality = random.uniform(0.6, 1.0)\n",
    "    \n",
    "    status = \"success\" if (error_rate <= config.max_error_rate and \n",
    "                         avg_quality >= config.min_quality_score) else \"failed\"\n",
    "    \n",
    "    result = DatabricksJobResult(\n",
    "        job_name=config.job_name,\n",
    "        status=status,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        records_processed=processed_records,\n",
    "        records_failed=failed_records,\n",
    "        processing_time_seconds=processing_time,\n",
    "        average_quality_score=avg_quality,\n",
    "        error_rate=error_rate,\n",
    "        output_location=config.output_path,\n",
    "        error_message=\"Quality threshold not met\" if status == \"failed\" else None\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example Databricks job configuration\n",
    "job_config = DatabricksConfig(\n",
    "    job_name=\"customer_data_processing\",\n",
    "    cluster_size=\"medium\",\n",
    "    timeout_minutes=120,\n",
    "    input_path=\"/dbfs/raw/customers/\",\n",
    "    output_path=\"/dbfs/processed/customers/\",\n",
    "    checkpoint_path=\"/dbfs/checkpoints/customers/\",\n",
    "    min_quality_score=0.8,\n",
    "    max_error_rate=0.05\n",
    ")\n",
    "\n",
    "print(\"Databricks Job Configuration:\")\n",
    "print(job_config.json(indent=2))\n",
    "\n",
    "# Simulate job execution\n",
    "print(\"\\nExecuting Databricks job...\")\n",
    "job_result = simulate_databricks_job(job_config)\n",
    "\n",
    "print(f\"\\nJob Result:\")\n",
    "print(f\"Status: {job_result.status}\")\n",
    "print(f\"Records processed: {job_result.records_processed:,}\")\n",
    "print(f\"Records failed: {job_result.records_failed:,}\")\n",
    "print(f\"Success rate: {job_result.success_rate:.1%}\")\n",
    "print(f\"Processing time: {job_result.processing_time_seconds:.2f} seconds\")\n",
    "print(f\"Average quality score: {job_result.average_quality_score:.3f}\")\n",
    "print(f\"Error rate: {job_result.error_rate:.1%}\")\n",
    "\n",
    "if job_result.error_message:\n",
    "    print(f\"Error: {job_result.error_message}\")\n",
    "\n",
    "# Job result as JSON (for logging/monitoring)\n",
    "print(\"\\nJob Result JSON:\")\n",
    "print(job_result.json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with Databricks Delta Lake\n",
    "\n",
    "Use Pydantic models with Delta Lake for schema evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaTableSchema(BaseModel):\n",
    "    \"\"\"Schema definition for Delta tables\"\"\"\n",
    "    table_name: str\n",
    "    database_name: str = \"default\"\n",
    "    schema_version: str = \"1.0\"\n",
    "    \n",
    "    # Table properties\n",
    "    partition_columns: List[str] = []\n",
    "    z_order_columns: List[str] = []\n",
    "    table_properties: Dict[str, str] = {}\n",
    "    \n",
    "    # Schema fields\n",
    "    fields: List[Dict[str, str]] = []\n",
    "    \n",
    "    # Data quality constraints\n",
    "    check_constraints: List[str] = []\n",
    "    \n",
    "    @validator('table_name', 'database_name')\n",
    "    def validate_names(cls, v):\n",
    "        if not v.replace('_', '').replace('-', '').isalnum():\n",
    "            raise ValueError('Name must be alphanumeric with underscores/hyphens')\n",
    "        return v.lower()\n",
    "\n",
    "def create_delta_schema_from_pydantic(model_class: BaseModel) -> DeltaTableSchema:\n",
    "    \"\"\"Generate Delta table schema from Pydantic model\"\"\"\n",
    "    schema = model_class.schema()\n",
    "    \n",
    "    fields = []\n",
    "    constraints = []\n",
    "    \n",
    "    for field_name, field_info in schema['properties'].items():\n",
    "        # Map Pydantic types to Spark SQL types\n",
    "        spark_type = map_pydantic_to_spark_type(field_info)\n",
    "        \n",
    "        # Check if field is required\n",
    "        nullable = field_name not in schema.get('required', [])\n",
    "        \n",
    "        field_def = {\n",
    "            'name': field_name,\n",
    "            'type': spark_type,\n",
    "            'nullable': str(nullable).lower()\n",
    "        }\n",
    "        \n",
    "        fields.append(field_def)\n",
    "        \n",
    "        # Add constraints based on Pydantic field definitions\n",
    "        field_constraints = extract_field_constraints(field_name, field_info)\n",
    "        constraints.extend(field_constraints)\n",
    "    \n",
    "    table_name = model_class.__name__.lower().replace('raw', '').replace('features', '')\n",
    "    \n",
    "    delta_schema = DeltaTableSchema(\n",
    "        table_name=table_name,\n",
    "        fields=fields,\n",
    "        check_constraints=constraints,\n",
    "        table_properties={\n",
    "            'delta.autoOptimize.optimizeWrite': 'true',\n",
    "            'delta.autoOptimize.autoCompact': 'true'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return delta_schema\n",
    "\n",
    "def map_pydantic_to_spark_type(field_info: Dict) -> str:\n",
    "    \"\"\"Map Pydantic field type to Spark SQL type\"\"\"\n",
    "    field_type = field_info.get('type', 'string')\n",
    "    \n",
    "    type_mapping = {\n",
    "        'integer': 'bigint',\n",
    "        'number': 'double',\n",
    "        'string': 'string',\n",
    "        'boolean': 'boolean',\n",
    "        'array': 'array<string>',  # Simplified\n",
    "        'object': 'string',  # JSON string\n",
    "    }\n",
    "    \n",
    "    # Handle date/datetime types\n",
    "    if field_info.get('format') == 'date-time':\n",
    "        return 'timestamp'\n",
    "    elif field_info.get('format') == 'date':\n",
    "        return 'date'\n",
    "    \n",
    "    return type_mapping.get(field_type, 'string')\n",
    "\n",
    "def extract_field_constraints(field_name: str, field_info: Dict) -> List[str]:\n",
    "    \"\"\"Extract check constraints from Pydantic field info\"\"\"\n",
    "    constraints = []\n",
    "    \n",
    "    # Minimum value constraints\n",
    "    if 'minimum' in field_info:\n",
    "        constraints.append(f\"{field_name} >= {field_info['minimum']}\")\n",
    "    \n",
    "    # Maximum value constraints\n",
    "    if 'maximum' in field_info:\n",
    "        constraints.append(f\"{field_name} <= {field_info['maximum']}\")\n",
    "    \n",
    "    # String length constraints\n",
    "    if 'minLength' in field_info:\n",
    "        constraints.append(f\"length({field_name}) >= {field_info['minLength']}\")\n",
    "    \n",
    "    if 'maxLength' in field_info:\n",
    "        constraints.append(f\"length({field_name}) <= {field_info['maxLength']}\")\n",
    "    \n",
    "    # Pattern constraints\n",
    "    if 'pattern' in field_info:\n",
    "        # Note: This would need proper regex translation\n",
    "        constraints.append(f\"{field_name} RLIKE '{field_info['pattern']}'\")\n",
    "    \n",
    "    return constraints\n",
    "\n",
    "def generate_delta_ddl(schema: DeltaTableSchema) -> str:\n",
    "    \"\"\"Generate Delta table DDL from schema\"\"\"\n",
    "    \n",
    "    # Create table statement\n",
    "    ddl = f\"CREATE TABLE IF NOT EXISTS {schema.database_name}.{schema.table_name} (\\n\"\n",
    "    \n",
    "    # Add fields\n",
    "    field_definitions = []\n",
    "    for field in schema.fields:\n",
    "        nullable = \"\" if field['nullable'] == 'true' else \" NOT NULL\"\n",
    "        field_definitions.append(f\"  {field['name']} {field['type']}{nullable}\")\n",
    "    \n",
    "    ddl += \",\\n\".join(field_definitions)\n",
    "    ddl += \"\\n)\"\n",
    "    \n",
    "    # Add partitioning\n",
    "    if schema.partition_columns:\n",
    "        ddl += f\"\\nPARTITIONED BY ({', '.join(schema.partition_columns)})\"\n",
    "    \n",
    "    # Add table properties\n",
    "    if schema.table_properties:\n",
    "        props = [f\"'{k}' = '{v}'\" for k, v in schema.table_properties.items()]\n",
    "        ddl += f\"\\nTBLPROPERTIES ({', '.join(props)})\"\n",
    "    \n",
    "    # Add constraints\n",
    "    if schema.check_constraints:\n",
    "        ddl += \"\\n\\n-- Check Constraints\\n\"\n",
    "        for i, constraint in enumerate(schema.check_constraints):\n",
    "            ddl += f\"ALTER TABLE {schema.database_name}.{schema.table_name} ADD CONSTRAINT check_{i} CHECK ({constraint});\\n\"\n",
    "    \n",
    "    return ddl\n",
    "\n",
    "# Generate Delta schemas from our Pydantic models\n",
    "customer_schema = create_delta_schema_from_pydantic(CustomerRaw)\n",
    "feature_schema = create_delta_schema_from_pydantic(CustomerFeatures)\n",
    "\n",
    "print(\"Delta Lake Schema for CustomerRaw:\")\n",
    "print(customer_schema.json(indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DELTA TABLE DDL for CustomerRaw:\")\n",
    "print(\"=\"*60)\n",
    "customer_ddl = generate_delta_ddl(customer_schema)\n",
    "print(customer_ddl)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DELTA TABLE DDL for CustomerFeatures:\")\n",
    "print(\"=\"*60)\n",
    "feature_ddl = generate_delta_ddl(feature_schema)\n",
    "print(feature_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This notebook demonstrated comprehensive Databricks integration patterns with Pydantic:\n",
    "\n",
    "### Key Integration Patterns:\n",
    "1. **Data Validation**: Robust input validation for ETL pipelines\n",
    "2. **Feature Engineering**: Type-safe feature transformations\n",
    "3. **Quality Monitoring**: Automated data quality checks\n",
    "4. **ML Feature Stores**: Validated ML-ready feature vectors\n",
    "5. **Job Configuration**: Type-safe job configuration and results\n",
    "6. **Delta Lake Integration**: Schema evolution and constraint management\n",
    "\n",
    "### Best Practices for Databricks + Pydantic:\n",
    "\n",
    "#### 1. **Schema Management**\n",
    "- Use Pydantic models as single source of truth for schemas\n",
    "- Generate Delta table schemas from Pydantic models\n",
    "- Version your data models for schema evolution\n",
    "\n",
    "#### 2. **Data Quality**\n",
    "- Implement validation at ingestion time\n",
    "- Separate valid/invalid records for processing\n",
    "- Monitor data quality metrics continuously\n",
    "\n",
    "#### 3. **Error Handling**\n",
    "- Gracefully handle validation errors\n",
    "- Log detailed error information for debugging\n",
    "- Implement retry logic for transient failures\n",
    "\n",
    "#### 4. **Performance**\n",
    "- Batch validation for better performance\n",
    "- Use appropriate data types for Spark\n",
    "- Consider validation overhead in job planning\n",
    "\n",
    "#### 5. **Monitoring**\n",
    "- Track validation success rates\n",
    "- Monitor feature drift and quality scores\n",
    "- Set up alerting for quality thresholds\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement these patterns in your Databricks workspaces\n",
    "2. Create reusable libraries for common validation patterns\n",
    "3. Set up CI/CD pipelines for model versioning\n",
    "4. Integrate with MLflow for model and feature tracking\n",
    "5. Build automated data quality dashboards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}